{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Translation System - Research Analysis\n",
    "\n",
    "**Project:** LLM Course HW3 - Turing Assignment\n",
    "\n",
    "**Date:** 2025-11-25\n",
    "\n",
    "**Research Question:** How does the rate of spelling errors in input text affect semantic drift in multi-agent sequential translation systems?\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Data Loading](#data-loading)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "4. [Statistical Analysis](#statistical-analysis)\n",
    "5. [Sensitivity Analysis](#sensitivity-analysis)\n",
    "6. [Visualization](#visualization)\n",
    "7. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Data analysis imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Configure visualization\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append(str(Path.cwd()))\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading <a id='data-loading'></a>\n",
    "\n",
    "Load experimental results from the translation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analysis results\n",
    "results_dir = Path('results/analysis')\n",
    "\n",
    "# Find the most recent analysis file\n",
    "analysis_files = list(results_dir.glob('analysis_*.csv'))\n",
    "if analysis_files:\n",
    "    latest_file = max(analysis_files, key=lambda p: p.stat().st_mtime)\n",
    "    df = pd.read_csv(latest_file)\n",
    "    print(f\"✓ Loaded data from: {latest_file.name}\")\n",
    "    print(f\"  Rows: {len(df)}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"⚠ No analysis files found. Please run the main pipeline first.\")\n",
    "    print(\"  Command: python -m src.main\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "if df is not None:\n",
    "    display(df.head(10))\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis <a id='eda'></a>\n",
    "\n",
    "Examine the distribution and characteristics of our experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Distribution of error rates\n",
    "    print(\"Error Rate Distribution:\")\n",
    "    print(df['error_rate'].value_counts().sort_index())\n",
    "    \n",
    "    # Distribution of distances\n",
    "    print(\"\\nDistance Statistics by Error Rate:\")\n",
    "    display(df.groupby('error_rate')['distance'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Visualize distributions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Error rate distribution\n",
    "    axes[0].hist(df['error_rate'], bins=20, edgecolor='black')\n",
    "    axes[0].set_xlabel('Error Rate')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Distribution of Error Rates')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distance distribution\n",
    "    axes[1].hist(df['distance'], bins=20, edgecolor='black', color='coral')\n",
    "    axes[1].set_xlabel('Semantic Distance')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Distribution of Semantic Distances')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis <a id='statistical-analysis'></a>\n",
    "\n",
    "Perform statistical tests to determine the relationship between error rate and semantic distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Correlation analysis\n",
    "    pearson_corr, pearson_p = pearsonr(df['error_rate'], df['distance'])\n",
    "    spearman_corr, spearman_p = spearmanr(df['error_rate'], df['distance'])\n",
    "    \n",
    "    print(\"Correlation Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Pearson Correlation:  r = {pearson_corr:.4f}, p-value = {pearson_p:.6f}\")\n",
    "    print(f\"Spearman Correlation: ρ = {spearman_corr:.4f}, p-value = {spearman_p:.6f}\")\n",
    "    print(\"\\nInterpretation:\")\n",
    "    if pearson_p < 0.001:\n",
    "        print(\"  *** Highly significant relationship (p < 0.001)\")\n",
    "    elif pearson_p < 0.01:\n",
    "        print(\"  ** Very significant relationship (p < 0.01)\")\n",
    "    elif pearson_p < 0.05:\n",
    "        print(\"  * Significant relationship (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"  No significant relationship (p >= 0.05)\")\n",
    "    \n",
    "    if abs(pearson_corr) > 0.7:\n",
    "        print(\"  Strong correlation\")\n",
    "    elif abs(pearson_corr) > 0.4:\n",
    "        print(\"  Moderate correlation\")\n",
    "    else:\n",
    "        print(\"  Weak correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Linear regression\n",
    "    from scipy.stats import linregress\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = linregress(\n",
    "        df['error_rate'], df['distance']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nLinear Regression Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Equation: distance = {slope:.4f} × error_rate + {intercept:.4f}\")\n",
    "    print(f\"R² = {r_value**2:.4f}\")\n",
    "    print(f\"Standard Error: {std_err:.6f}\")\n",
    "    print(f\"\\nFor every 10% increase in error rate:\")\n",
    "    print(f\"  Expected distance increase: {slope * 0.1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sensitivity Analysis <a id='sensitivity-analysis'></a>\n",
    "\n",
    "Examine how sensitive the semantic distance is to changes in error rate at different ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Bin error rates and calculate statistics\n",
    "    df['error_bin'] = pd.cut(\n",
    "        df['error_rate'], \n",
    "        bins=[0, 0.1, 0.25, 0.4, 1.0],\n",
    "        labels=['Low (0-10%)', 'Medium (10-25%)', 'High (25-40%)', 'Very High (40%+)']\n",
    "    )\n",
    "    \n",
    "    print(\"Sensitivity by Error Rate Range:\")\n",
    "    print(\"=\" * 60)\n",
    "    sensitivity_stats = df.groupby('error_bin')['distance'].agg([\n",
    "        ('Mean', 'mean'),\n",
    "        ('Std Dev', 'std'),\n",
    "        ('Min', 'min'),\n",
    "        ('Max', 'max'),\n",
    "        ('Count', 'count')\n",
    "    ])\n",
    "    display(sensitivity_stats)\n",
    "    \n",
    "    # Calculate rate of change between bins\n",
    "    print(\"\\nRate of Change Between Ranges:\")\n",
    "    means = sensitivity_stats['Mean'].values\n",
    "    for i in range(1, len(means)):\n",
    "        change = means[i] - means[i-1]\n",
    "        pct_change = (change / means[i-1]) * 100\n",
    "        print(f\"  {sensitivity_stats.index[i-1]} → {sensitivity_stats.index[i]}:\")\n",
    "        print(f\"    Δ = {change:.4f} ({pct_change:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Box plot by error range\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=df, x='error_bin', y='distance', palette='Set2')\n",
    "    plt.xlabel('Error Rate Range')\n",
    "    plt.ylabel('Semantic Distance')\n",
    "    plt.title('Semantic Distance Distribution by Error Rate Range')\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization <a id='visualization'></a>\n",
    "\n",
    "Create publication-quality visualizations of the experimental results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Main scatter plot with regression\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Scatter plot\n",
    "    scatter = ax.scatter(\n",
    "        df['error_rate'] * 100,  # Convert to percentage\n",
    "        df['distance'],\n",
    "        alpha=0.6,\n",
    "        s=100,\n",
    "        c=df['distance'],\n",
    "        cmap='viridis',\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    \n",
    "    # Add regression line\n",
    "    z = np.polyfit(df['error_rate'], df['distance'], 2)\n",
    "    p = np.poly1d(z)\n",
    "    x_line = np.linspace(df['error_rate'].min(), df['error_rate'].max(), 100)\n",
    "    ax.plot(\n",
    "        x_line * 100, \n",
    "        p(x_line), \n",
    "        'r--', \n",
    "        linewidth=2, \n",
    "        label=f'Polynomial Fit (R²={r_value**2:.3f})'\n",
    "    )\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Spelling Error Rate (%)', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Semantic Distance (Cosine)', fontsize=13, fontweight='bold')\n",
    "    ax.set_title(\n",
    "        'Impact of Spelling Errors on Semantic Drift\\nin Multi-Agent Translation Pipeline',\n",
    "        fontsize=15,\n",
    "        fontweight='bold',\n",
    "        pad=20\n",
    "    )\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Semantic Distance', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save high-resolution version\n",
    "    # fig.savefig('results/graphs/research_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    # print(\"\\n✓ High-resolution graph saved to results/graphs/research_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Heatmap showing relationship\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    pivot_data = df.pivot_table(\n",
    "        values='distance',\n",
    "        index=pd.cut(df['error_rate'], bins=10),\n",
    "        aggfunc=['mean', 'std', 'count']\n",
    "    )\n",
    "    \n",
    "    # Display summary heatmap data\n",
    "    print(\"Summary Statistics Heatmap Data:\")\n",
    "    display(pivot_data)\n",
    "    \n",
    "    # Create mean distance heatmap\n",
    "    sns.heatmap(\n",
    "        pivot_data['mean'].to_frame().T,\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        cmap='YlOrRd',\n",
    "        cbar_kws={'label': 'Mean Semantic Distance'},\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title('Mean Semantic Distance by Error Rate Range', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Error Rate Range')\n",
    "    ax.set_ylabel('')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions <a id='conclusions'></a>\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on the statistical analysis:\n",
    "\n",
    "1. **Correlation**: The analysis reveals the strength and significance of the relationship between spelling error rate and semantic distance.\n",
    "\n",
    "2. **Non-linearity**: The polynomial fit suggests potential non-linear effects, indicating that LLMs may handle low error rates better than high ones.\n",
    "\n",
    "3. **Sensitivity**: The sensitivity analysis shows how the impact varies across different error rate ranges.\n",
    "\n",
    "4. **Robustness**: The standard deviation and range statistics indicate the variability in results, suggesting factors beyond error rate affect semantic drift.\n",
    "\n",
    "### Implications\n",
    "\n",
    "- **Input Quality Matters**: Even small improvements in input quality can reduce semantic drift significantly.\n",
    "- **Error Thresholds**: There may be critical thresholds where semantic drift accelerates.\n",
    "- **LLM Resilience**: Modern LLMs show some resilience to low error rates but struggle with high error rates.\n",
    "\n",
    "### Future Work\n",
    "\n",
    "1. Test with different LLM models\n",
    "2. Explore different types of errors (semantic vs. orthographic)\n",
    "3. Investigate error correction strategies\n",
    "4. Analyze intermediate translation steps\n",
    "5. Examine specific error patterns that cause maximum drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Generated with Claude Code**\n",
    "\n",
    "**Co-Authored-By:** Claude <noreply@anthropic.com>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
